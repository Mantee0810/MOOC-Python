标题
一种基于多层次残差连接的图像分类算法研究


摘要
计算机视觉是借鉴人类的感官活动机理，用计算机模拟人类的视觉感官，从而获取物体和环境信息。作为计算机视觉的主要任务之一，图像分类不仅能够被用于人脸识别，交通场景识别，图像检索与照片自动归类，还是目标检测，图像分割的理论基础。本文利用现有的CNN架构网络——即ConvNeXt【14】，通过对网络残差连接和卷积结构进行调整修改，实现了分类精度和推理速度的平衡，在准确率基本不变的前提下，能够同时降低计算量与显存占用，更好地推进网络轻量化。


1简介
	为了完成图像分类等视觉识别任务，EfficientNet【13】提及，如今的卷积神经网络正在变得更深、更宽、更高分辨率，因此总是伴随着几百个卷积层和几千个通道，通常涉及高达上亿次的运算，从而算机硬件有着越来越高的要求。
在图像分类问题中，我们知道横向和纵向内容互有关联而又不尽相同，为了更好地提取到底层抽象信息，ERFNet【12】将二维卷积核替换为两个一维卷积核，实现信息的分步提取。本文在原ConvNeXt网络的基础上将深度卷积拆分，并通过卷积核裂变操作实现速度和准确度的统一。
另一方面，resnet【5】和densenet【6】在图像分类任务上取得的成功让我们看到了捷径分支的巨大潜力，本文提出了一种多层次捷径连接的结构，在上述两个网络的基础上进行跳层连接，以帮助信息在网络内部更好地流通。
我们在imagenet1k【11】的分类任务上评估了模型，控制变量法下的一系列实验显示了网络结构的有效性和更好的性能。与基线网络ConvNeXt相比，我们的网络允许信息在多个不同的特征通道流通，达到了准确率与推理速度的平衡，在FLOPs和准确率基本不变的前提下，大幅降低了对显存的占用，同时还发现一种降低同结构网络输出差异性的方法。 


2相关工作
图像分类是计算机视觉中最经典的任务之一，既可以用于人脸识别，给公安、国防和金融部门提供生物信息数据，也可以用在交通领域，实现场景理解和分割，为自动驾驶的研究打下基础。
图像分类方法有手工提取特征和机器学习特征两种，在深度学习出现之前，图像分类通过具体计算提取特征，综合利用主成分分析技术【1】和Fisher线性判别【15】对图像进行全局描述。即先采用主成分分析技术对输入进行降维操作，进行噪声过滤和数据拟合，再利用图像的标签信息，通过Fisher线性判别最大化距离和标准差的比值，以求解分类问题。但由于时间长、准确率低、上限不高等原因，在Alex【2】的深层卷积神经网络崭露头角之后，传统的分类方法日渐式微。
自此，人们普遍认为更深的网络就会带来更好的效果，GoogLeNet【3】和VGG【4】正是基于这种思潮发展起来的。其中，Christian Szegedy通过对Inception结构的设计，人为构建稀疏连接，引入多尺度感受野和多尺度融合；Karen Simonyan∗ & Andrew Zisserman则通过更小的卷积核与更多的卷积层，稳步提高了网络深度。这两个典型网络提高了图像识别准确率的上限，也吸引了更多的人投入该领域继续研究。
	单纯的提高网络深度会产生诸如“梯度消失”、“梯度爆炸”等问题，这些问题本质上都是因为反向传播过程中进行多次连乘，从而导致权值更新不稳定。ResNet【5】和DenseNet【6】都是通过添加“捷径”连接，在不改变参数量的前提下减缓“梯度消失”现象。其中，ResNet由于其达到了性能和效果上的平衡，现已经成为许多网络设计的基准网络。
	随着便携式电子设备（手机，平板电脑等）的发展，人们又将研究方向转向了轻量化模型。MobileNet【7】和ShuffleNet【8】都是人们在轻量化上做出的成果。Andrew G. Howard将Alex Krizhevsky提出的组卷积应用到了极致，提出了深度卷积，极大降低了训练的参数量；Xiangyu Zhang在MobileNet的基础上，提出了通道洗牌的观点，加强了信息在网络内部的流通与交互；Eduardo Romera【12】则将原方形卷积替换为两个条形卷积，分别提取横向和纵向信息。
	在transformer【9】的流行下，传统卷积神经网络遭遇挑战，2022年，Zhuang Liu【10】等人通过对swim transformer的模仿，对传统CNN结构进行大量微调，提出ConvNeXt网络，又一次突破了ImageNet【11】数据集中准确率的上限。

3融合卷积网络
3.1多层次捷径连接结构
	在ResNet、DenseNet和ConvNeXt中，捷径分支都存在于单个Block中，不同的Stage仍是串联拼接的，这给特征图的复用带来了麻烦：统一的串联拼接会削弱捷径分支的能力，呈几何倍数增长的分支又极大提高了对显存的占用。本文提出多层次捷径连接结构，在原本ConvNeXt结构的基础上添加了从下采样到Stage输出端的直连通路，图1对比了DenseNet和ResNet的原始网络结构流程图，图2绘制的是ConvNeXt-tiny与Ours-tiny的网络流程图，图3以每个Stage中含有3个Block为例，从左往右分别展现了ConvNeXt、DenseNet、ResNet、Ours网络结构中的Stage结构：

图1 DenseNet和ResNet的网络流程图


图2 ConvNeXt-tiny与Ours-tiny的网络流程图

图3 ConvNeXt、DenseNet、ResNet、Ours的Stage结构对比

通过对图1进行分析，我们不难看出，ResNet和DenseNet网络中的残差结构都局限在单个Block中，一旦我们将完整的Stage抽象为一个整体，那么在网络的流程图中展现的就是一种纯粹的串联结构。
在图2中，Ours的网络流程图展现的结构是：在原本已经具有残差连接结构的Block基础上，给每一个Stage结构增加了另一层残差连接，我们称之为“多层次捷径连接”。这种结构能够保证特征图的信息实现跨Stage的流通，避免在后续的特征提取与融合操作时丢失原始的特征图信息。
通过图3可以看出，三种网络均在Block中添加了残差连接，Ours所做的更新不在Block层面，而在Stage与整体网络结构层面。以包含三个Block的一个Stage为例，我们可以通过图4来更精准地描述这种结构：

图4 对不同网络中Stage结构的详细描述

多层次捷径连接能够更好的允许数据在不同的网络层级中流通，并且由于同步池化设计平衡了捷径连接的数目，相比起其他主流网络，能够有效地降低硬件中的显存占用，实现更高性价比。

3.2卷积核拆分与裂变
	现代神经网络通常通过升维与降维的组合，平衡计算量与模型性能【5,7】。传统网络中使用的卷积多为方形卷积，本网络尝试将方形卷积替换为两个条形卷积，感受野不变，且两层卷积比单层卷积增加了一层非线性变换，网络的表达能力得到增强。另一方面，3*3的方形卷积对每个单独像素点考虑了周围八个临近像素点的影响，容易引起“虚假相关性”现象，网络过于关注上下文信息，从而可能从周边背景中推断出图上未曾直接出现的物体。
本文不仅将原7*7卷积改为3*3卷积，还把3*3卷积再裂变为非对称的3*1和1*3卷积的串联，力图在减少计算量的同时提高网络的表达能力。图5详细地描述了如何用分解卷积代替传统卷积。

图5 分解卷积

对于一个卷积核大小为7*7，通道数为dim的深度卷积，其参数量可由公式1计算得出： 
    〖kernal size〗^2*in channel*out channel*num
=7^2*1*1*dim
=49*dim 
另一方面，将普通7*7卷积裂变为1*7和7*1的一维卷积后，其参数量可由公式2计算得出：
     kernal size vertical*kernal size horizontal*in channel*out channel*num*2
=7*1*1*1*dim*2
=14*dim 
可以直观地看出，通过对深度卷积核进行拆分和裂变，能够将卷积部分参数量降至原卷积核的28.57%，同时在两层结构之间增加了非线性表达，更注重对横向信息和纵向信息的利用，并且极大减小了网络规模。

3.3多层次残差连接与分解卷积的综合应用网络
	本文对前人的工作进行了两处修改，一方面通过增添多层次残差连接结构，实现了数据信息在不同阶段的交互；另一方面通过卷积核的拆分与裂变，减少了模型的参数量，并且降低了网络的规模。图6完整展示了本文所提出的网络结构：

图6 Ours网络的完整结构，其中n1、n2、n3和n4是可以选择的Block数量

4实验
	我们主要在ImageNet 2012【11】分类数据集上评估模型，该实验仅为验证网络结构的有效性，为了减小计算量，我们从1k个类别中随机选取300个作为训练与测试的样本。为了确保模型的可靠性，所有实验均沿用了ConvNeXt的预训练方式和学习率衰减策略。

4.1消融实验
	本网络的核心思想在多层次捷径连接和卷积的拆分与裂变，我们将对这两点分别进行消融实验并评估。所有实验数据均来自单个NVIDIA GeForce RTX3090设备。

4.1.1多层次捷径连接网络的消融研究
	为了分析多层次捷径连接的有效性，我们同时比较了ConvNeXt基线网络和添加多层次捷径连接后的网络在迭代次数为30轮时的准确率，参数量和FLOPs，如表1所示：

表1 多层次残差连接的消融实验（其中acc1是最高概率结果为真实标签的概率，acc5是前五个概率结果中包含真实标签的概率）

图7 差异化对比
	从表1中我们能够看出，多层次捷径连接是本质其实就是残差连接，即添加该结构并不会影响网络的参数量和计算量，但在执行反向传播时该结构能够有效缓解“梯度消失”现象。
另外，图7展现出了不同网络之间准确度的差异。增加多层次捷径连接后，浅层网络对网络输出的影响会变得更大，使得深层网络对结果只能进行小范围的微调，削弱了结构相同、大小不同的网络之间的差异，便于在部署时选择更轻量化的网络（牺牲更少的性能）。

表2记录了在300类数据集中增添多层次残差连接前后的测试时间和单图速率，我们能看到，尽管在上述训练阶段，增添的结构会使得网络整体的训练时长提高，但在测试阶段，修改后的网络比原网络有着更快的测试速度。

表2 测试时间（平均时间为测试时每一张图片的测试时间）
	从表8我们能够看出，在准确率少许降低的前提下，本文提出的网络在单图测试速率上有着更好的性能，对于三种不同大小的网络，速率对比原网络分别提升了5.19%、5.36%和4.08%，在small大小的网络上改动有着更为明显的提高。

4.1.2卷积核拆分与裂变的消融研究
	为了证明卷积核拆分与裂变的性能，利用7*7深度卷积，3*3规则卷积，3*3深度卷积和两个条形卷积进行对比实验，我们对比了这四种卷积在迭代30次时的准确率，参数量，FLOPs，最高显存占用和训练时长，如表3所示：

表3 不同卷积核的消融实验
	从表3中能够看出，采用3*3分解卷积能够在计算量、参数量、最大显存占用和训练时长上都得到明显的提升。
对比起原网络的深度卷积，在计算量层面，采用分解卷积会使得tiny大小的网络降低6.71%，small大小的网络降低6.12%，base大小的网络降低4.68%，即网络结构越小，分解卷积对计算量的优化效果越好；在参数量方面，采用分解卷积会使得tiny大小的网络降低3.06%，small大小的网络降低3.84%，base大小的网络降低2.95%，即在small大小的网络结构中，分解卷积对参数量的优化效果达到最优；在最大显存占用方面，采用分解卷积会使得tiny大小的网络降低21.45%，small大小的网络降低8.92%，base大小的网络降低1.41%，即网络结构越小，分解卷积对最大显存占用的优化效果越好；在训练时长方面，采用分解卷积会使得tiny大小的网络降低17.83%，small大小的网络降低6.73%，base大小的网络降低12.53%，即在tiny大小的网络结构中，分解卷积对训练时长的优化效果达到最优。
	
4.2与ResNet、DenseNet的比较
4.2.1准确率对比
	ResNet和DenseNet作为残差连接网络的提出者和佼佼者，我们还对比了我们的网络结构与这两者之间的差异，实验迭代次数均为30次时，具体结果如表4所示：

表4 三种网络对比
	从表4中能够看出，我们的网络比起各种不同大小的ResNet和DenseNet，都得到了一定程度的提高，acc1最大提高了10.56%，最低提高了2.91%；acc5最大提高了6.41%，最低提高了1.45%，即本文提出的算法对于acc1的提高会比对acc5的提高更加明显。

4.2.2显存占用对比
	我们知道，特征图在计算机中留存的时间越长，对显存的占用就越大。基于此，我们的网络在特征复用的基础上，平衡了特征图留存时长和准确率的关系，在单次迭代状态下，我们对比了批次大小分别为32,64,128时，三者之间的显存占用情况，如表5所示：

表5 不同网络的显存占用
从表5中能够看出，我们的网络在显存占用和计算量上都是最优的，和各种不同的网络相比，在显存占用上比DenseNet121有着最大的提升：对比起经典网络ResNet50，在批次为32时，最大显存占用降低了23.34%；在批次为64时，最大显存占用降低了23.43%；在批次为128时，最大显存占用降低了24.37%。从数据中能够发现，本文提出的算法在批次更大时，对最大显存占用有着更好的性能提高。
不同网络的全过程显存占用如图8所示：
图8 从上往下分别是ConvNeXt、DenseNet、ResNet和Ours四种网络，从左往右分别是batch size取值为32、64和128的情况


本文所使用的网络能够对ImageNet数据集进行图像分类，图9展示了部分分类结果：
图9 网络分类后的部分结果展示（每一行前的编号为代码中数据集的分类名）
	图9是本次实验中部分分类正确的结果，其中每一行都对应的是不同类别的物体。由于图中物体尺寸较大，位置较为居中，使其在多次池化中仍能保证较高的识别度。


总结与展望
	我们在原基线网络的基础上，提出了多层次残差连接，同时为了保证网络的直连特点，还对连接部分进行了同步池化操作，能让特征图的信息多次以原始形态传递到后续训练中，另一方面，我们还对卷积结构进行了调整，在保证准确率的前提下尽可能使得网络更加轻量化并减少对显存的占用。
	本文对神经网络的结构进行了调整，又一次利用了残差结构对网络进行优化，后续会尝试将这种调整应用在目标识别、图像分割等细分领域，也希望这种多层次残差连接的结构能给后续研究者一些启发，以此实现一些更先进的功能。

参考文献




